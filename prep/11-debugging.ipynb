{"cells": [{"cell_type": "markdown", "attachments": {}, "source": "---  \n# Debugging  \nteaching: 30  \nexercises: 20  \nquestions:  \n- \"How can I debug my program?\"  \nobjectives:  \n- \"Debug code containing an error systematically.\"  \n- \"Identify ways of making code less error-prone and more easily tested.\"  \nkeypoints:  \n- \"Know what code is supposed to do *before* trying to debug it.\"  \n- \"Make it fail every time.\"  \n- \"Make it fail fast.\"  \n- \"Change one thing at a time, and for a reason.\"  \n- \"Keep track of what you've done.\"  \n- \"Be humble.\"  \n---  \n\nOnce testing has uncovered problems,  \nthe next step is to fix them.  \nMany novices do this by making more-or-less random changes to their code  \nuntil it seems to produce the right answer,  \nbut that's very inefficient  \n(and the result is usually only correct for the one case they're testing).  \nThe more experienced a programmer is,  \nthe more systematically they debug,  \nand most follow some variation on the rules explained below.  \n\n## Know What It's Supposed to Do  \n\nThe first step in debugging something is to  \n*know what it's supposed to do*.  \n\"My program doesn't work\" isn't good enough:  \nin order to diagnose and fix problems,  \nwe need to be able to tell correct output from incorrect.  \nIf we can write a test case for the failing case --- i.e.,  \nif we can assert that with *these* inputs,  \nthe function should produce *that* result ---  \nthen we're ready to start debugging.  \nIf we can't,  \nthen we need to figure out how we're going to know when we've fixed things.  \n\nBut writing test cases for scientific software is frequently harder than  \nwriting test cases for commercial applications,  \nbecause if we knew what the output of the scientific code was supposed to be,  \nwe wouldn't be running the software:  \nwe'd be writing up our results and moving on to the next program.  \nIn practice,  \nscientists tend to do the following:  \n\n1.  *Test with simplified data.*  \n    Before doing statistics on a real data set,  \n    we should try calculating statistics for a single record,  \n    for two identical records,  \n    for two records whose values are one step apart,  \n    or for some other case where we can calculate the right answer by hand.  \n\n2.  *Test a simplified case.*  \n    If our program is supposed to simulate  \n    magnetic eddies in rapidly-rotating blobs of supercooled helium,  \n    our first test should be a blob of helium that isn't rotating,  \n    and isn't being subjected to any external electromagnetic fields.  \n    Similarly,  \n    if we're looking at the effects of climate change on speciation,  \n    our first test should hold temperature, precipitation, and other factors constant.  \n\n3.  *Compare to an oracle.*  \n    A <span style=\"color:red\" title=\"A program, device, data set, or human being  \nagainst which the results of a test can be compared.\">test oracle</span>  \n    such as experimental data, an older program, or a human expert.  \n    We use test oracles to determine if our new program produces the correct results.  \n    If we have a test oracle,  \n    we should store its output for particular cases  \n    so that we can compare it with our new results as often as we like  \n    without re-running that program.  \n\n4.  *Check conservation laws.*  \n    Mass, energy, and other quantities are conserved in physical systems,  \n    so they should be in programs as well.  \n    Similarly,  \n    if we are analyzing patient data,  \n    the number of records should either stay the same or decrease  \n    as we move from one analysis to the next  \n    (since we might throw away outliers or records with missing values).  \n    If \"new\" patients start appearing out of nowhere as we move through our pipeline,  \n    it's probably a sign that something is wrong.  \n\n5.  *Visualize.*  \n    Data analysts frequently use simple visualizations to check both  \n    the science they're doing  \n    and the correctness of their code  \n    (just as we did in the opening lesson of this tutorial).  \n    This should only be used for debugging as a last resort,  \n    though,  \n    since it's very hard to compare two visualizations automatically.  \n\n## Make It Fail Every Time  \n\nWe can only debug something when it fails,  \nso the second step is always to find a test case that  \n*makes it fail every time*.  \nThe \"every time\" part is important because  \nfew things are more frustrating than debugging an intermittent problem:  \nif we have to call a function a dozen times to get a single failure,  \nthe odds are good that we'll scroll past the failure when it actually occurs.  \n\nAs part of this,  \nit's always important to check that our code is \"plugged in\",  \ni.e.,  \nthat we're actually exercising the problem that we think we are.  \nEvery programmer has spent hours chasing a bug,  \nonly to realize that they were actually calling their code on the wrong data set  \nor with the wrong configuration parameters,  \nor are using the wrong version of the software entirely.  \nMistakes like these are particularly likely to happen when we're tired,  \nfrustrated,  \nand up against a deadline,  \nwhich is one of the reasons late-night (or overnight) coding sessions  \nare almost never worthwhile.  \n\n## Make It Fail Fast  \n\nIf it takes 20 minutes for the bug to surface,  \nwe can only do three experiments an hour.  \nThis means that we'll get less data in more time and that  \nwe're more likely to be distracted by other things as we wait for our program to fail,  \nwhich means the time we *are* spending on the problem is less focused.  \nIt's therefore critical to *make it fail fast*.  \n\nAs well as making the program fail fast in time,  \nwe want to make it fail fast in space,  \ni.e.,  \nwe want to localize the failure to the smallest possible region of code:  \n\n1.  The smaller the gap between cause and effect,  \n    the easier the connection is to find.  \n    Many programmers therefore use a divide and conquer strategy to find bugs,  \n    i.e.,  \n    if the output of a function is wrong,  \n    they check whether things are OK in the middle,  \n    then concentrate on either the first or second half,  \n    and so on.  \n\n2.  N things can interact in N! different ways,  \n    so every line of code that *isn't* run as part of a test  \n    means more than one thing we don't need to worry about.  \n\n## Change One Thing at a Time, For a Reason  \n\nReplacing random chunks of code is unlikely to do much good.  \n(After all,  \nif you got it wrong the first time,  \nyou'll probably get it wrong the second and third as well.)  \nGood programmers therefore  \n*change one thing at a time, for a reason*.  \nThey are either trying to gather more information  \n(\"is the bug still there if we change the order of the loops?\")  \nor test a fix  \n(\"can we make the bug go away by sorting our data before processing it?\").  \n\nEvery time we make a change,  \nhowever small,  \nwe should re-run our tests immediately,  \nbecause the more things we change at once,  \nthe harder it is to know what's responsible for what  \n(those N! interactions again).  \nAnd we should re-run *all* of our tests:  \nmore than half of fixes made to code introduce (or re-introduce) bugs,  \nso re-running all of our tests tells us whether we have regressed.  \n\n## Keep Track of What You've Done  \n\nGood scientists keep track of what they've done  \nso that they can reproduce their work,  \nand so that they don't waste time repeating the same experiments  \nor running ones whose results won't be interesting.  \nSimilarly,  \ndebugging works best when we  \n*keep track of what we've done*  \nand how well it worked.  \nIf we find ourselves asking,  \n\"Did left followed by right with an odd number of lines cause the crash?  \nOr was it right followed by left?  \nOr was I using an even number of lines?\"  \nthen it's time to step away from the computer,  \ntake a deep breath,  \nand start working more systematically.  \n\nRecords are particularly useful when the time comes to ask for help.  \nPeople are more likely to listen to us  \nwhen we can explain clearly what we did,  \nand we're better able to give them the information they need to be useful.  \n", "metadata": {}}, {"cell_type": "markdown", "attachments": {}, "source": "> ## Version Control Revisited  \n>  \n> Version control is often used to reset software to a known state during debugging,  \n> and to explore recent changes to code that might be responsible for bugs.  \n> In particular,  \n> most version control systems (e.g. git, Mercurial) have:  \n> 1. a `blame` command that shows who last changed each line of a file;  \n> 2. a `bisect` command that helps with finding the commit that introduced an  \n>    issue.  \n", "metadata": {}}, {"cell_type": "markdown", "attachments": {}, "source": "## Be Humble  \n\nAnd speaking of help:  \nif we can't find a bug in 10 minutes,  \nwe should *be humble* and ask for help.  \nExplaining the problem to someone else is often useful,  \nsince hearing what we're thinking helps us spot inconsistencies and hidden assumptions.  \nIf you don't have someone nearby to share your problem description with, get a  \n[rubber duck](https://en.wikipedia.org/wiki/Rubber_duck_debugging)  \n\nAsking for help also helps alleviate confirmation bias.  \nIf we have just spent an hour writing a complicated program,  \nwe want it to work,  \nso we're likely to keep telling ourselves why it should,  \nrather than searching for the reason it doesn't.  \nPeople who aren't emotionally invested in the code can be more objective,  \nwhich is why they're often able to spot the simple mistakes we have overlooked.  \n\nPart of being humble is learning from our mistakes.  \nProgrammers tend to get the same things wrong over and over:  \neither they don't understand the language and libraries they're working with,  \nor their model of how things work is wrong.  \nIn either case,  \ntaking note of why the error occurred  \nand checking for it next time  \nquickly turns into not making the mistake at all.  \n\nAnd that is what makes us most productive in the long run.  \nAs the saying goes,  \n*A week of hard work can sometimes save you an hour of thought*.  \nIf we train ourselves to avoid making some kinds of mistakes,  \nto break our code into modular, testable chunks,  \nand to turn every assumption (or mistake) into an assertion,  \nit will actually take us *less* time to produce working programs,  \nnot more.  \n", "metadata": {}}, {"cell_type": "markdown", "attachments": {}, "source": "## Debug With a Neighbor  \n\nTake a function that you have written today, and introduce a tricky bug.  \nYour function should still run, but will give the wrong output.  \nSwitch seats with your neighbor and attempt to debug  \nthe bug that they introduced into their function.  \nWhich of the principles discussed above did you find helpful?  \n", "metadata": {}}, {"cell_type": "markdown", "attachments": {}, "source": "## Not Supposed to be the Same  \n\nYou are assisting a researcher with Python code that computes the  \nBody Mass Index (BMI) of patients.  The researcher is concerned because  \nall patients seemingly have unusual and identical BMIs, despite having different  \nphysiques.  BMI is calculated as **weight in kilograms**  \ndivided by the square of **height in metres**.  \n\nUse the debugging principles in this exercise and locate problems  \nwith the code. What suggestions would you give the researcher for  \nensuring any later changes they make work correctly?  \n\n---  \n```python\npatients = [[70, 1.8], [80, 1.9], [150, 1.7]]  \n\ndef calculate_bmi(weight, height):  \nreturn weight / (height ** 2)  \n\nfor patient in patients:  \nweight, height = patients[0]  \nbmi = calculate_bmi(height, weight)  \nprint(\"Patient's BMI is: %f\" % bmi)  \n```\n---  \n\n\n---  \n```python\nPatient's BMI is: 0.000367  \nPatient's BMI is: 0.000367  \nPatient's BMI is: 0.000367  \n```\n---  \n\n\n<details>  \n<summary><b> Solution</b></summary>  \n\n* The loop is not being utilised correctly. `height` and `weight` are always  \nset as the first patient's data during each iteration of the loop.  \n\n* The height/weight variables are reversed in the function call to  \n`calculate_bmi(...)`, the correct BMIs are 21.604938, 22.160665 and 51.903114.  \n</details>  \n", "metadata": {}}], "metadata": {}, "nbformat": 4, "nbformat_minor": 0}